{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import copy\n",
    "import re\n",
    "import string\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 5566\n",
    "MAX_POSITIONS_LEN = 500\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_text(text):\n",
    "    #punctuation_remove = string.punctuation.replace(\"!\", \"\")\n",
    "    # text = re.sub(r'([a-z])\\1\\1\\1+', r'\\1\\1\\1', text)\n",
    "    # text = re.sub(r'http : bit . ly', '', text)\n",
    "    # text = re.sub(r'http : tinyurl . com', '', text)\n",
    "    # text = re.sub(r'http : cli . gs', '', text)\n",
    "    # text = re.sub(r'http : www', '', text)\n",
    "    # text = \"\".join([i for i in text if i not in punctuation_remove]).strip()\n",
    "    # text = \" \".join(text.split()) #remove abundant white space\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_train_label(path='train_label.csv'):\n",
    "    tra_lb_pd = pd.read_csv(path)\n",
    "    label = torch.FloatTensor(tra_lb_pd['label'].values)\n",
    "    idx = tra_lb_pd['id'].tolist()\n",
    "    text = [parsing_text(s).split(' ') for s in tra_lb_pd['text'].tolist()]\n",
    "    return idx, text, label\n",
    "\n",
    "def load_test(path='test.csv'):\n",
    "    tst_pd = pd.read_csv(path)\n",
    "    idx = tst_pd['id'].tolist()\n",
    "    text = [parsing_text(s).split(' ') for s in tst_pd['text'].tolist()]\n",
    "    return idx, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, sentences, w2v_config):\n",
    "        self.sentences = sentences\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "        self.w2v_model = None\n",
    "        self.build_word2vec(sentences, **w2v_config)\n",
    "        \n",
    "        \n",
    "    def build_word2vec(self, x, path, dim, window, min_count, iter, sg, hs, negative):\n",
    "        if os.path.isfile(path):\n",
    "            print(\"loading word2vec model ...\")\n",
    "            w2v_model = Word2Vec.load(path)\n",
    "        else:\n",
    "            print(\"training word2vec model ...\")\n",
    "            w2v_model = Word2Vec(\n",
    "                x, size=dim, window=window, min_count=min_count, \n",
    "                workers=12, iter=iter, sg=sg, hs = hs,\n",
    "                negative = negative)\n",
    "        self.w2v_model = w2v_model\n",
    "            \n",
    "        self.embedding_dim = w2v_model.vector_size\n",
    "        for i, word in enumerate(w2v_model.wv.vocab):\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(w2v_model.wv[word])\n",
    "        \n",
    "        self.embedding_matrix = torch.tensor(np.array(self.embedding_matrix))\n",
    "        self.add_embedding('<PAD>')\n",
    "        self.add_embedding('<UNK>')\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        \n",
    "    def add_embedding(self, word):\n",
    "        # 把 word 加進 embedding，並賦予他一個隨機生成的 representation vector\n",
    "        # word 只會是 \"<PAD>\" 或 \"<UNK>\"\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)   \n",
    "        \n",
    "    def sentence2idx(self, sentence):\n",
    "        sentence_idx = []\n",
    "        for word in sentence:\n",
    "            if word in self.word2idx.keys():\n",
    "                sentence_idx.append(self.word2idx[word])\n",
    "            else:\n",
    "                sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
    "        return torch.LongTensor(np.array(sentence_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, id_list, sentences, labels, preprocessor):\n",
    "        self.id_list = id_list\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.preprocessor = preprocessor\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is None: \n",
    "            return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx])\n",
    "        return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx]), self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        id_list = torch.LongTensor([d[0] for d in data])\n",
    "        #lengths: sentence lengths of each sentence\n",
    "        lengths = torch.LongTensor([len(d[1]) for d in data])\n",
    "        #pad_sequence to maximum length in each batch\n",
    "        texts = pad_sequence(\n",
    "            [d[1] for d in data], \n",
    "            batch_first=True).contiguous()\n",
    "\n",
    "        if self.labels is None: \n",
    "            return id_list, lengths, texts\n",
    "        \n",
    "        labels = torch.FloatTensor([d[2] for d in data])\n",
    "        return id_list, lengths, texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Backbone(torch.nn.Module):\n",
    "    def __init__(self, embedding, hidden_dim, num_layers, bidirectional, fix_embedding=True):\n",
    "        super(LSTM_Backbone, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(embedding.size(1), hidden_dim, num_layers=num_layers, \\\n",
    "                                  bidirectional=bidirectional, batch_first=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        #output: output_state, (hidden_state, cell_state)\n",
    "        #output_state shape: (N,L,D∗H​)\n",
    "        output, (hidden, cell) = self.lstm(inputs)\n",
    "        #concat two hidden state of the bi-LSTM in the last layer\n",
    "        a,b,c,d = hidden[0], hidden[1], hidden[2], hidden[3]\n",
    "        out = torch.cat((c,d),1) #size = (N, hidden_size * 2)\n",
    "        return out\n",
    "    \n",
    "class Header(torch.nn.Module):\n",
    "    def __init__(self, dropout, hidden_dim):\n",
    "        super(Header, self).__init__()\n",
    "        self.fc1 = torch.nn.Sequential(torch.nn.Dropout(dropout),\n",
    "                                        torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "                                        torch.nn.LeakyReLU(negative_slope=0.05),\n",
    "                                        torch.nn.BatchNorm1d(hidden_dim),\n",
    "                                        torch.nn.Dropout(dropout))\n",
    "\n",
    "        self.fc2 = torch.nn.Sequential(torch.nn.Linear(hidden_dim, 1),\n",
    "                                         torch.nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, inputs, lengths):\n",
    "        out = self.fc1(inputs)\n",
    "        out = self.fc2(out).squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, backbone, header, optimizer, criterion, device, epoch):\n",
    "\n",
    "    total_loss = []\n",
    "    total_acc = []\n",
    "    for i, (idx_list, lengths, texts, labels) in enumerate(train_loader):\n",
    "        lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if not backbone is None:\n",
    "            #output shape: (batch_size, max_sequence_length, hidden layer size)\n",
    "            inputs = backbone(inputs)\n",
    "        soft_predicted = header(inputs, lengths)\n",
    "        loss = criterion(soft_predicted, labels)\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            hard_predicted = (soft_predicted >= 0.5).int()\n",
    "            correct = sum(hard_predicted == labels).item()\n",
    "            acc = correct * 100 / len(labels)\n",
    "            total_acc.append(acc)\n",
    "\n",
    "    return np.mean(total_loss), np.mean(total_acc)\n",
    "\n",
    "def valid(valid_loader, backbone, header, criterion, device, epoch):\n",
    "    backbone.eval()\n",
    "    header.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = []\n",
    "        total_acc = []\n",
    "        \n",
    "        for i, (idx_list, lengths, texts, labels) in enumerate(valid_loader):\n",
    "            lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)\n",
    "\n",
    "            if not backbone is None:\n",
    "                inputs = backbone(inputs)\n",
    "            soft_predicted = header(inputs, lengths)\n",
    "            loss = criterion(soft_predicted, labels)\n",
    "            total_loss.append(loss.item())\n",
    "            \n",
    "            hard_predicted = (soft_predicted >= 0.5).int()\n",
    "            correct = sum(hard_predicted == labels).item()\n",
    "            acc = correct * 100 / len(labels)\n",
    "            total_acc.append(acc)\n",
    "            \n",
    "            #print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f}'.format(epoch+1, np.mean(total_loss), np.mean(total_acc)), end='\\r')\n",
    "    backbone.train()\n",
    "    header.train()\n",
    "    return np.mean(total_loss), np.mean(total_acc)\n",
    "\n",
    "            \n",
    "def run_training(train_loader, valid_loader, backbone, header, epoch_num, lr, patience, device, model_dir): \n",
    "\n",
    "    def no_improve(best_acc, best_loss, valid_loss, valid_acc):\n",
    "        return (best_acc > valid_acc) and (best_loss < valid_loss)\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    cnt_no_improve = 0\n",
    "    change_embedding = False\n",
    "    patience = patience\n",
    "\n",
    "    best_backbone = None\n",
    "    best_header = None\n",
    "\n",
    "    if backbone is None:\n",
    "        trainable_paras = header.parameters()\n",
    "    else:\n",
    "        trainable_paras = list(backbone.parameters()) + list(header.parameters())\n",
    "        \n",
    "    optimizer = torch.optim.Adam(trainable_paras, lr=lr)\n",
    "    \n",
    "    backbone.train()\n",
    "    header.train()\n",
    "    backbone = backbone.to(device)\n",
    "    header = header.to(device)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(epoch_num):\n",
    "        train_loss, train_acc = train(train_loader, backbone, header, optimizer, criterion, device, epoch)\n",
    "        print('Epoch {:} [Train] loss:{:.3f} acc:{:.3f} /'.\\\n",
    "            format(epoch+1, train_loss, train_acc), end = \" \")\n",
    "        valid_loss, valid_acc = valid(valid_loader, backbone, header, criterion, device, epoch)\n",
    "        end = time.time()\n",
    "        elapsed_minutes = (end - start) / 60\n",
    "        print('[Valid] loss:{:.3f} acc:{:.3f}, {:.2f} minutes elapsed'.\\\n",
    "            format(valid_loss, valid_acc, elapsed_minutes))\n",
    "        \n",
    "        if no_improve(best_acc, best_loss, valid_loss, valid_acc):\n",
    "            cnt_no_improve += 1\n",
    "        else:\n",
    "            best_acc = valid_acc\n",
    "            best_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "            cnt_no_improve = 0\n",
    "            best_backbone = copy.deepcopy(backbone)\n",
    "            best_header = copy.deepcopy(header)\n",
    "\n",
    "        if cnt_no_improve == patience:\n",
    "            if change_embedding is False:\n",
    "                print(\"Embedding now become tunable\")\n",
    "                #change embedding one time\n",
    "                backbone.embedding.weight.requires_grad = True\n",
    "                change_embedding = True\n",
    "                cnt_no_improve = 0\n",
    "                patience = 3\n",
    "            else:\n",
    "                print(\"Model result: Epoch {:}, [Valid] loss:{:.3f} acc:{:.3f}\".\\\n",
    "                    format(best_epoch+1, best_loss, best_acc))\n",
    "                return best_backbone, best_header, best_acc, best_loss\n",
    "    return best_backbone, best_header, best_acc, best_loss\n",
    "    \n",
    "def run_testing(test_loader, backbone, header, device, output_path):\n",
    "    with open(output_path, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['id', 'label'])\n",
    "        backbone.eval()\n",
    "        header.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (idx_list, lengths, texts) in enumerate(test_loader):\n",
    "                lengths, inputs = lengths.to(device), texts.to(device)\n",
    "                if not backbone is None:\n",
    "                    inputs = backbone(inputs)\n",
    "                soft_predicted = header(inputs, lengths)\n",
    "                hard_predicted = (soft_predicted >= 0.5).int()\n",
    "                for i, p in zip(idx_list, hard_predicted):\n",
    "                    writer.writerow([str(i.item()), str(p.item())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word2vec model ...\n",
      "total words: 30885\n",
      "Predicting testing dataset...\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = 'best_RNN_model.pth'\n",
    "W2V_DIR = 'best_w2v_model.model'\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "checkpoint = torch.load(MODEL_DIR)\n",
    "best_backbone = checkpoint[\"backbone\"]\n",
    "best_header = checkpoint[\"header\"]\n",
    "best_backbone = best_backbone.to(device)\n",
    "best_header = best_header.to(device)\n",
    "\n",
    "#prepare dataset\n",
    "test_idx, test_text = load_test('test.csv')\n",
    "\n",
    "param_grid = {\n",
    "    'dim': 256, 'hs': 0, \n",
    "    'iter': 2, 'min_count': 2, \n",
    "    'negative': 5, 'path': W2V_DIR, \n",
    "    'sg': 1, 'window': 5}\n",
    "\n",
    "preprocessor = Preprocessor(None, param_grid)\n",
    "test_dataset = TwitterDataset(test_idx, test_text, None, preprocessor)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = False,\n",
    "                                            collate_fn = test_dataset.collate_fn,\n",
    "                                            num_workers = 2)\n",
    "print(\"Predicting testing dataset...\")\n",
    "run_testing(test_loader, best_backbone, best_header, device, 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': 256, 'hs': 0, 'iter': 2, 'min_count': 5, 'negative': 5, 'path': '-', 'sg': 1, 'window': 5}\n",
      "training word2vec model ...\n",
      "total words: 14826\n",
      "Building LSTM model...\n",
      "Epoch 1 [Train] loss:0.521 acc:74.027 / [Valid] loss:0.483 acc:76.717, 0.38 minutes elapsed\n",
      "Epoch 2 [Train] loss:0.473 acc:77.456 / [Valid] loss:0.466 acc:77.562, 0.77 minutes elapsed\n",
      "Epoch 3 [Train] loss:0.454 acc:78.582 / [Valid] loss:0.448 acc:78.760, 1.17 minutes elapsed\n",
      "Epoch 4 [Train] loss:0.442 acc:79.258 / [Valid] loss:0.437 acc:79.588, 1.57 minutes elapsed\n",
      "Epoch 5 [Train] loss:0.428 acc:79.948 / [Valid] loss:0.433 acc:79.891, 1.97 minutes elapsed\n",
      "Epoch 6 [Train] loss:0.415 acc:80.770 / [Valid] loss:0.435 acc:79.741, 2.40 minutes elapsed\n",
      "Embedding now become non-tunable\n",
      "Epoch 7 [Train] loss:0.395 acc:82.137 / [Valid] loss:0.404 acc:81.613, 2.84 minutes elapsed\n",
      "Epoch 8 [Train] loss:0.319 acc:86.150 / [Valid] loss:0.443 acc:81.030, 3.27 minutes elapsed\n",
      "Epoch 9 [Train] loss:0.237 acc:90.103 / [Valid] loss:0.514 acc:79.631, 3.71 minutes elapsed\n",
      "Epoch 10 [Train] loss:0.165 acc:93.367 / [Valid] loss:0.658 acc:78.866, 4.16 minutes elapsed\n",
      "Model result: Epoch 7, [Valid] loss:0.404 acc:81.613\n",
      "Tuning result:\n",
      "Best param sets: {'dim': 256, 'hs': 0, 'iter': 2, 'min_count': 5, 'negative': 5, 'path': '-', 'sg': 1, 'window': 5}\n",
      "Best loss 0.4036660009995103, best acc 81.61295572916666\n"
     ]
    }
   ],
   "source": [
    "w2v_param_grid = {\n",
    "    'dim': [256], 'hs': [0], \n",
    "    'iter': [2], 'min_count': [5], \n",
    "    'negative': [5], 'path': [\"-\"], \n",
    "    'sg': [1], 'window': [5]}\n",
    "\n",
    "lstm_config = {\n",
    "    'hidden_dim': 256, 'num_layers': 2, \n",
    "    'bidirectional': True, 'fix_embedding': True}\n",
    "\n",
    "header_config = {\n",
    "    'dropout': 0.5, \n",
    "    'hidden_dim': 512}\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "MAX_POSITIONS_LEN = 500\n",
    "MODEL_DIR = \"best_RNN_model.pth\"\n",
    "W2V_DIR = \"best_w2v_model.model\"\n",
    "EPOCH_NUM = 100\n",
    "\n",
    "lr = 0.001\n",
    "patience = 1\n",
    "\n",
    "global_best_backbone = None\n",
    "global_best_header = None\n",
    "global_best_acc = 0\n",
    "global_best_loss = np.inf\n",
    "global_best_preprocessor = None\n",
    "global_best_params = None\n",
    "\n",
    "for param_grid in ParameterGrid(w2v_param_grid):\n",
    "    print(param_grid)\n",
    "    train_idx, train_label_text, label = load_train_label('train_label.csv')\n",
    "    test_idx, test_text = load_test('test.csv')\n",
    "    preprocessor = Preprocessor(train_label_text, param_grid)\n",
    "    train_idx, valid_idx, train_label_text, valid_label_text, train_label, valid_label = train_test_split(train_idx, train_label_text, label, test_size=0.12)\n",
    "    train_dataset, valid_dataset = TwitterDataset(train_idx, train_label_text, train_label, preprocessor), TwitterDataset(valid_idx, valid_label_text, valid_label, preprocessor)\n",
    "    test_dataset = TwitterDataset(test_idx, test_text, None, preprocessor)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                                batch_size = BATCH_SIZE,\n",
    "                                                shuffle = True,\n",
    "                                                collate_fn = train_dataset.collate_fn,\n",
    "                                                num_workers = 2)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset = valid_dataset,\n",
    "                                                batch_size = BATCH_SIZE,\n",
    "                                                shuffle = False,\n",
    "                                                collate_fn = valid_dataset.collate_fn,\n",
    "                                                num_workers = 2)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                                batch_size = BATCH_SIZE,\n",
    "                                                shuffle = False,\n",
    "                                                collate_fn = test_dataset.collate_fn,\n",
    "                                               num_workers = 2)\n",
    "\n",
    "    print(\"Building LSTM model...\")\n",
    "    backbone = LSTM_Backbone(preprocessor.embedding_matrix, **lstm_config)\n",
    "    header = Header(**header_config)\n",
    "    local_best_backbone, local_best_header, local_best_acc, local_best_loss = \\\n",
    "        run_training(\n",
    "        train_loader, valid_loader, backbone, header, \n",
    "        EPOCH_NUM, lr, patience, device, MODEL_DIR)\n",
    "    if local_best_acc > global_best_acc and local_best_loss < global_best_loss:\n",
    "        global_best_backbone = local_best_backbone\n",
    "        global_best_header = local_best_header\n",
    "        global_best_preprocessor = preprocessor\n",
    "        global_best_params = param_grid\n",
    "        \n",
    "        #update global loss and acc\n",
    "        global_best_acc = local_best_acc\n",
    "        global_best_loss = local_best_loss \n",
    "\n",
    "\n",
    "print(\"Tuning result:\")\n",
    "print(f\"Best param sets: {global_best_params}\")\n",
    "print(f\"Best loss {global_best_loss}, best acc {global_best_acc}\")\n",
    "torch.save(\n",
    "    {'backbone': global_best_backbone, 'header': global_best_header}, \n",
    "    MODEL_DIR)\n",
    "global_best_preprocessor.w2v_model.save(W2V_DIR)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
