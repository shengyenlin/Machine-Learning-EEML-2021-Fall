{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_NUM = 2\n",
    "BATCH_SIZE = 128\n",
    "EPOCH_NUM = 50\n",
    "MAX_POSITIONS_LEN = 500\n",
    "SEED = 5566\n",
    "MODEL_DIR = 'model.pth'\n",
    "lr = 0.001\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "torch.cuda.set_device(DEVICE_NUM)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "w2v_config = {'path': 'w2v.model', 'dim': 256}\n",
    "lstm_config = {'hidden_dim': 256, 'num_layers': 2, 'bidirectional': True, 'fix_embedding': True}\n",
    "header_config = {'dropout': 0.5, 'hidden_dim': 512}\n",
    "assert header_config['hidden_dim'] == lstm_config['hidden_dim'] or header_config['hidden_dim'] == lstm_config['hidden_dim'] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_text(text):\n",
    "    # TODO: do data processing\n",
    "    return text\n",
    "\n",
    "def load_train_label(path='train_label.csv'):\n",
    "    tra_lb_pd = pd.read_csv(path)\n",
    "    label = torch.FloatTensor(tra_lb_pd['label'].values)\n",
    "    idx = tra_lb_pd['id'].tolist()\n",
    "    text = [parsing_text(s).split(' ') for s in tra_lb_pd['text'].tolist()]\n",
    "    return idx, text, label\n",
    "\n",
    "def load_train_nolabel(path='train_nolabel.csv'):\n",
    "    tra_nlb_pd = pd.read_csv(path)\n",
    "    text = [parsing_text(s).split(' ') for s in tra_nlb_pd['text'].tolist()]\n",
    "    return text\n",
    "\n",
    "def load_test(path='test.csv'):\n",
    "    tst_pd = pd.read_csv(path)\n",
    "    idx = tst_pd['id'].tolist()\n",
    "    text = [parsing_text(s).split(' ') for s in tst_pd['text'].tolist()]\n",
    "    return idx, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, sentences, w2v_config):\n",
    "        self.sentences = sentences\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "        self.build_word2vec(sentences, **w2v_config)\n",
    "        \n",
    "    def build_word2vec(self, x, path, dim):\n",
    "        if os.path.isfile(path):\n",
    "            print(\"loading word2vec model ...\")\n",
    "            w2v_model = Word2Vec.load(path)\n",
    "        else:\n",
    "            print(\"training word2vec model ...\")\n",
    "            w2v_model = Word2Vec(x, size=dim, window=5, min_count=2, workers=12, iter=2, sg=1)\n",
    "            print(\"saving word2vec model ...\")\n",
    "            w2v_model.save(path)\n",
    "            \n",
    "        self.embedding_dim = w2v_model.vector_size\n",
    "        for i, word in enumerate(w2v_model.wv.vocab):\n",
    "            #e.g. self.word2index['he'] = 1 \n",
    "            #e.g. self.index2word[1] = 'he'\n",
    "            #e.g. self.vectors[1] = 'he' vector\n",
    "            \n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(w2v_model[word])\n",
    "        \n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        self.add_embedding('<PAD>')\n",
    "        self.add_embedding('<UNK>')\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        \n",
    "    def add_embedding(self, word):\n",
    "        # 把 word 加進 embedding，並賦予他一個隨機生成的 representation vector\n",
    "        # word 只會是 \"<PAD>\" 或 \"<UNK>\"\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)   \n",
    "        \n",
    "    def sentence2idx(self, sentence):\n",
    "        sentence_idx = []\n",
    "        for word in sentence:\n",
    "            if word in self.word2idx.keys():\n",
    "                sentence_idx.append(self.word2idx[word])\n",
    "            else:\n",
    "                sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
    "        return torch.LongTensor(sentence_idx)\n",
    "    \n",
    "class TwitterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, id_list, sentences, labels, preprocessor):\n",
    "        self.id_list = id_list\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.preprocessor = preprocessor\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is None: return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx])\n",
    "        return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx]), self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        id_list = torch.LongTensor([d[0] for d in data])\n",
    "        lengths = torch.LongTensor([len(d[1]) for d in data])\n",
    "        texts = pad_sequence(\n",
    "            [d[1] for d in data], batch_first=True).contiguous()\n",
    "     \n",
    "        if self.labels is None: \n",
    "            return id_list, lengths, texts\n",
    "        \n",
    "        labels = torch.FloatTensor([d[2] for d in data])\n",
    "        return id_list, lengths, texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, train_label_text, label = load_train_label('data/train_label.csv')\n",
    "preprocessor = Preprocessor(train_label_text, w2v_config)\n",
    "\n",
    "train_idx, valid_idx, train_label_text, valid_label_text, train_label, valid_label = train_test_split(train_idx, train_label_text, label, test_size=0.12)\n",
    "train_dataset, valid_dataset = TwitterDataset(train_idx, train_label_text, train_label, preprocessor), TwitterDataset(valid_idx, valid_label_text, valid_label, preprocessor)\n",
    "\n",
    "test_idx, test_text = load_test('data/test.csv')\n",
    "test_dataset = TwitterDataset(test_idx, test_text, None, preprocessor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = True,\n",
    "                                            collate_fn = train_dataset.collate_fn,\n",
    "                                            num_workers = 8)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset = valid_dataset,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = False,\n",
    "                                            collate_fn = valid_dataset.collate_fn,\n",
    "                                            num_workers = 8)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = False,\n",
    "                                            collate_fn = valid_dataset.collate_fn,\n",
    "                                            num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Backbone(torch.nn.Module):\n",
    "    def __init__(self, embedding, hidden_dim, num_layers, bidirectional, fix_embedding=True):\n",
    "        super(LSTM_Backbone, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(embedding.size(1), hidden_dim, num_layers=num_layers, \\\n",
    "                                  bidirectional=bidirectional, batch_first=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        x, _ = self.lstm(inputs)\n",
    "        return x\n",
    "    \n",
    "class Header(torch.nn.Module):\n",
    "    def __init__(self, dropout, hidden_dim):\n",
    "        super(Header, self).__init__()\n",
    "        # TODO: you should design your classifier module\n",
    "        self.classifier = torch.nn.Sequential(torch.nn.Dropout(dropout),\n",
    "                                         torch.nn.Linear(hidden_dim, 1),\n",
    "                                         torch.nn.Sigmoid())\n",
    "        \n",
    "    @ torch.no_grad()\n",
    "    def _get_length_masks(self, lengths):\n",
    "        # lengths: (batch_size, ) in cuda\n",
    "        ascending = torch.arange(MAX_POSITIONS_LEN)[:lengths.max().item()].unsqueeze(\n",
    "            0).expand(len(lengths), -1).to(lengths.device)\n",
    "        length_masks = (ascending < lengths.unsqueeze(-1)).unsqueeze(-1)\n",
    "        return length_masks\n",
    "    \n",
    "    def forward(self, inputs, lengths):\n",
    "        # the input shape should be (N, L, D∗H)\n",
    "        pad_mask = self._get_length_masks(lengths)\n",
    "        inputs = inputs * pad_mask\n",
    "        inputs = inputs.sum(dim=1)\n",
    "        out = self.classifier(inputs).squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, backbone, header, optimizer, criterion, device, epoch):\n",
    "\n",
    "    total_loss = []\n",
    "    total_acc = []\n",
    "    \n",
    "    for i, (idx_list, lengths, texts, labels) in enumerate(train_loader):\n",
    "        lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if not backbone is None:\n",
    "            inputs = backbone(inputs)\n",
    "        soft_predicted = header(inputs, lengths)\n",
    "        loss = criterion(soft_predicted, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            hard_predicted = (soft_predicted >= 0.5).int()\n",
    "            correct = sum(hard_predicted == labels).item()\n",
    "            batch_size = len(labels)\n",
    "        \n",
    "        print('[ Epoch {}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(epoch+1, i+1, len(train_loader), loss.item(), correct * 100 / batch_size), end='\\r')\n",
    "\n",
    "def valid(valid_loader, backbone, header, criterion, device, epoch):\n",
    "    backbone.eval()\n",
    "    header.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = []\n",
    "        total_acc = []\n",
    "        \n",
    "        for i, (idx_list, lengths, texts, labels) in enumerate(valid_loader):\n",
    "            lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)\n",
    "\n",
    "            if not backbone is None:\n",
    "                inputs = backbone(inputs)\n",
    "            soft_predicted = header(inputs, lengths)\n",
    "            loss = criterion(soft_predicted, labels)\n",
    "            total_loss.append(loss.item())\n",
    "            \n",
    "            hard_predicted = (soft_predicted >= 0.5).int()\n",
    "            correct = sum(hard_predicted == labels).item()\n",
    "            acc = correct * 100 / len(labels)\n",
    "            total_acc.append(acc)\n",
    "            \n",
    "            print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f}'.format(epoch+1, np.mean(total_loss), np.mean(total_acc)), end='\\r')\n",
    "    backbone.train()\n",
    "    header.train()\n",
    "    return np.mean(total_loss), np.mean(total_acc)\n",
    "\n",
    "            \n",
    "def run_training(train_loader, valid_loader, backbone, header, epoch_num, lr, device, model_dir): \n",
    "    def check_point(backbone, header, loss, acc, model_dir):\n",
    "        # TODO\n",
    "        torch.save({'backbone': backbone, 'header': header}, model_dir)\n",
    "    def is_stop(loss, acc):\n",
    "        # TODO\n",
    "        return False\n",
    "    \n",
    "    if backbone is None:\n",
    "        trainable_paras = header.parameters()\n",
    "    else:\n",
    "        trainable_paras = list(backbone.parameters()) + list(header.parameters())\n",
    "        \n",
    "    optimizer = torch.optim.Adam(trainable_paras, lr=lr)\n",
    "    \n",
    "    backbone.train()\n",
    "    header.train()\n",
    "    backbone = backbone.to(device)\n",
    "    header = header.to(device)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    for epoch in range(epoch_num):\n",
    "        train(train_loader, backbone, header, optimizer, criterion, device, epoch)\n",
    "        loss, acc = valid(valid_loader, backbone, header, criterion, device, epoch)\n",
    "        print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f} '.format(epoch+1, loss, acc))\n",
    "        check_point(backbone, header, loss, acc, model_dir)\n",
    "        if is_stop(loss, acc):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = LSTM_Backbone(preprocessor.embedding_matrix, **lstm_config)\n",
    "header = Header(**header_config)\n",
    "\n",
    "run_training(train_loader, valid_loader, backbone, header, EPOCH_NUM, lr, device, MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_testing(test_loader, backbone, header, device, output_path):\n",
    "    with open(output_path, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['id', 'label'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (idx_list, lengths, texts) in enumerate(test_loader):\n",
    "                lengths, inputs = lengths.to(device), texts.to(device)\n",
    "                if not backbone is None:\n",
    "                    inputs = backbone(inputs)\n",
    "                soft_predicted = header(inputs, lengths)\n",
    "                hard_predicted = (soft_predicted >= 0.5).int()\n",
    "                for i, p in zip(idx_list, hard_predicted):\n",
    "                    writer.writerow([str(i.item()), str(p.item())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_testing(test_loader, backbone, header, device, 'pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data peaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
