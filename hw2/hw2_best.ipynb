{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B06702064 會計五 林聖硯\n",
    "# Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"train.csv\"\n",
    "path_test = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path_train, path_test):\n",
    "    data_train = pd.read_csv(path_train, skipinitialspace = True)\n",
    "    data_test = pd.read_csv(path_test, skipinitialspace = True)\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "def write_to_csv(y_pred, file_path):\n",
    "    with open(file_path, 'w', newline='') as csvf:\n",
    "        writer = csv.writer(csvf)\n",
    "        writer.writerow(['id','label'])\n",
    "        for i in range(int(y_pred.shape[0])):\n",
    "            writer.writerow([i + 1, int(y_pred[i])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.num_cols = [\"age\", \"fnlwgt\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "        self.order_cat_cols = [\"education_num\"]\n",
    "        self.cat_cols = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\n",
    "        self.target_ratio = dict()\n",
    "        self.robust_scaler = None\n",
    "\n",
    "    def transform_label(self, data_train):\n",
    "        label_dict = {'<=50K': 0, '>50K': 1}\n",
    "        data_train_ = data_train.copy()\n",
    "        data_train_[\"income\"] = data_train_[\"income\"].apply(lambda x: label_dict[x])\n",
    "        return data_train_ \n",
    "\n",
    "    def do_target_encoding(self, data, isTraining=False):\n",
    "        if isTraining:\n",
    "            for col in self.cat_cols:\n",
    "                #print(data[[col, \"income\"]].groupby([col]).mean())\n",
    "                self.target_ratio[col] = data[[col, \"income\"]].groupby([col]).mean()\n",
    "                data[col] = data[col].apply(lambda x: self.target_ratio[col][\"income\"][x])\n",
    "        else:\n",
    "            for col in self.cat_cols:\n",
    "                data[col] = data[col].apply(lambda x: self.target_ratio[col][\"income\"][x])\n",
    "        return data\n",
    "\n",
    "    def do_robust_scaling(self, data, isTraining=False):\n",
    "        if isTraining:\n",
    "            self.robust_scaler = RobustScaler()\n",
    "            self.robust_scaler.fit(data)   \n",
    "        data_scaled = self.robust_scaler.transform(data)\n",
    "        return data_scaled\n",
    "\n",
    "    def preprocess_train_data(self, data_train):\n",
    "        data_train = self.transform_label(data_train)     \n",
    "        X_train_order = np.array(data_train[self.order_cat_cols])\n",
    "        \n",
    "        X_train_num = np.array(data_train[self.num_cols])\n",
    "        X_train_num_scaled = self.do_robust_scaling(X_train_num, isTraining=True)\n",
    "\n",
    "        X_train_encoded = self.do_target_encoding(data_train, isTraining=True)\n",
    "        X_train_cat = np.array(X_train_encoded[self.cat_cols])\n",
    "        \n",
    "        #combine\n",
    "        X_train = np.concatenate([X_train_order, X_train_num_scaled, X_train_cat], axis = 1)\n",
    "        y_train = data_train[\"income\"]\n",
    "        return X_train, y_train\n",
    "\n",
    "    def preprocess_test_data(self, data_test):\n",
    "        X_test_order = np.array(data_test[self.order_cat_cols])\n",
    "        \n",
    "        X_test_num = np.array(data_test[self.num_cols])\n",
    "        X_test_num_scaled = self.do_robust_scaling(X_test_num, isTraining=False)\n",
    "\n",
    "        X_test_encoded = self.do_target_encoding(data_test, isTraining=False)\n",
    "        X_test_cat = np.array(X_test_encoded[self.cat_cols])\n",
    "        \n",
    "        #combine\n",
    "        X_test = np.concatenate([X_test_order, X_test_num_scaled, X_test_cat], axis = 1)\n",
    "        return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = load_data(path_train, path_test)\n",
    "DP = DataPreprocessor()\n",
    "X_train, y_train = DP.preprocess_train_data(data_train)\n",
    "X_test = DP.preprocess_test_data(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 250, num = 5)]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 10, num = 6)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "params_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'oob_score': [True]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed: 19.6min\n",
      "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed: 37.9min\n",
      "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed: 56.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False, random_state=201,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'max_depth': [5, 6, 7, 8, 9, 10],\n",
       "                         'min_samples_leaf': [1, 2, 4],\n",
       "                         'min_samples_split': [2, 5],\n",
       "                         'n_estimators': [100, 137, 175, 212, 250],\n",
       "                         'oob_score': [True]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC = RandomForestClassifier(random_state = 201)\n",
    "RFC_CV = GridSearchCV(estimator = RFC, param_grid = params_grid, cv = 5, verbose = 2, n_jobs = -1)\n",
    "RFC_CV.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8772150732471361"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC_CV.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = RFC_CV.predict(X_test)\n",
    "file_name = 'prediction.csv'\n",
    "write_to_csv(y_pred, file_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
